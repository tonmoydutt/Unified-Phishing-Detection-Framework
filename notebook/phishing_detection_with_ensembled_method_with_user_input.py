# -*- coding: utf-8 -*-
"""phishing_detection_with_ensembled_method_with_user_input.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XzUmRyqgnVneOxOfHnGprJR54GkQP5Rd
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.ensemble import VotingClassifier
from sklearn.metrics import accuracy_score, classification_report

data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Phishing_Email.csv')
data.head()

X = data['Email Text']
y = data['Email Type']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Replace NaNs with empty strings
X_train.fillna('', inplace=True)
X_test.fillna('', inplace=True)

# Vectorize the text data using TF-IDF
tfidf_vectorizer = TfidfVectorizer(max_features=5000)
X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)
X_test_tfidf = tfidf_vectorizer.transform(X_test)

knn_model = KNeighborsClassifier(n_neighbors=3)
xgb_model = XGBClassifier()
nn_model = MLPClassifier()
lr_model = LogisticRegression()
dt_model = DecisionTreeClassifier()
nb_model = MultinomialNB()

# Create a list of base models
estimators = [
    ('knn', knn_model),
    ('xgb', xgb_model),
    ('nn', nn_model),
    ('lr', lr_model),
    ('dt', dt_model),
    ('nb', nb_model)
]

# Create the ensemble model using VotingClassifier
voting_classifier = VotingClassifier(estimators=estimators, voting='soft')  # You can choose 'hard' or 'soft' voting

# Fit the ensemble model to your training data
voting_classifier.fit(X_train_tfidf, y_train)

# Evaluate the ensemble model on your test data
ensemble_accuracy = voting_classifier.score(X_test_tfidf, y_test)
print(f"Ensemble Model Accuracy: {ensemble_accuracy}")

from sklearn.metrics import accuracy_score, classification_report

from sklearn.preprocessing import LabelEncoder

# Encode class labels to integers
label_encoder = LabelEncoder()
y_train_encoded = label_encoder.fit_transform(y_train)
y_test_encoded = label_encoder.transform(y_test)


# Train multiple models
knn_model = KNeighborsClassifier(n_neighbors=3)
knn_model.fit(X_train_tfidf, y_train_encoded)

xgb_model = XGBClassifier()
xgb_model.fit(X_train_tfidf, y_train_encoded)

nn_model = MLPClassifier()
nn_model.fit(X_train_tfidf, y_train_encoded)

lr_model = LogisticRegression()
lr_model.fit(X_train_tfidf, y_train_encoded)

dt_model = DecisionTreeClassifier()
dt_model.fit(X_train_tfidf, y_train_encoded)

nb_model = MultinomialNB()
nb_model.fit(X_train_tfidf, y_train_encoded)

# Generate classification reports for individual models
def evaluate_model(model, X_test, y_test_encoded):
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test_encoded, y_pred)
    report = classification_report(y_test_encoded, y_pred, target_names=['Safe Email', 'Phishing Email'])
    return accuracy, report

knn_accuracy, knn_report = evaluate_model(knn_model, X_test_tfidf, y_test_encoded)
xgb_accuracy, xgb_report = evaluate_model(xgb_model, X_test_tfidf, y_test_encoded)
nn_accuracy, nn_report = evaluate_model(nn_model, X_test_tfidf, y_test_encoded)
lr_accuracy, lr_report = evaluate_model(lr_model, X_test_tfidf, y_test_encoded)
dt_accuracy, dt_report = evaluate_model(dt_model, X_test_tfidf, y_test_encoded)
nb_accuracy, nb_report = evaluate_model(nb_model, X_test_tfidf, y_test_encoded)

# Print the classification reports
print("K-Nearest Neighbors Classifier Report:\n", knn_report)
print("XGBoost Classifier Report:\n", xgb_report)
print("Multi-layer Perceptron Classifier Report:\n", nn_report)
print("Logistic Regression Classifier Report:\n", lr_report)
print("Decision Tree Classifier Report:\n", dt_report)
print("Naive Bayes Classifier Report:\n", nb_report)

print("K-Nearest Neighbors Model Accuracy:", knn_accuracy)
print("XGBoost Model Accuracy:", xgb_accuracy)
print("Neural Network Model Accuracy:", nn_accuracy)
print("Logistic Regression Model Accuracy:", lr_accuracy)
print("Decision Tree Model Accuracy:", dt_accuracy)
print("Naive Bayes Model Accuracy:", nb_accuracy)

from sklearn.ensemble import VotingClassifier
from sklearn.feature_extraction.text import TfidfVectorizer
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
import joblib
import numpy as np

max_words = 10000

# Create and fit the tokenizer
tokenizer = Tokenizer(num_words=max_words)
tokenizer.fit_on_texts(X_train)

# Save your ensemble model
joblib.dump(voting_classifier, 'voting_classifier.pkl')

# Save your TF-IDF vectorizer
joblib.dump(tfidf_vectorizer, 'tfidf_vectorizer.pkl')

# Save your Tokenizer
joblib.dump(tokenizer, 'tokenizer.pkl')


# Load your trained models, TF-IDF vectorizer, and Tokenizer
voting_classifier = joblib.load('voting_classifier.pkl')
tfidf_vectorizer = joblib.load('tfidf_vectorizer.pkl')
tokenizer = joblib.load('tokenizer.pkl')

# Load the max sequence length (you should have saved this value when training your models)
max_sequence_length = 200  # Replace with your actual value

# Function to preprocess user input and make predictions
def predict_email_type(user_input):
    # Preprocess user input
    user_input = [user_input]  # Put the user input into a list
    user_input_tfidf = tfidf_vectorizer.transform(user_input)
    user_input_seq = tokenizer.texts_to_sequences(user_input)
    user_input_pad = pad_sequences(user_input_seq, maxlen=max_sequence_length)

    # Make predictions using the ensemble model
    predicted_class = voting_classifier.predict(user_input_tfidf)

    print("Predicted Class:", predicted_class)  # Debugging line

    # Map the predicted class to the actual class names
    class_mapping = {0: 'Safe Email', 1: 'Phishing Email'}

    if predicted_class[0] in class_mapping:
        predicted_class_name = class_mapping[predicted_class[0]]
    else:
        predicted_class_name = "Unknown"

    return predicted_class_name


# Take user input and make predictions
while True:
    user_input = input("Enter an email text (or 'exit' to quit): ")
    if user_input.lower() == 'exit':
        break
    else:
        predicted_type = predict_email_type(user_input)
        # print(f"Predicted Email Type: {predicted_type}")

import matplotlib.pyplot as plt
import numpy as np

# Define the model names and their accuracies
model_names = ['KNN', 'XGBoost', 'Neural Network', 'Logistic Regression', 'Decision Tree', 'Naive Bayes']
accuracies = [knn_accuracy, xgb_accuracy, nn_accuracy, lr_accuracy, dt_accuracy, nb_accuracy]

# Create a bar plot
plt.figure(figsize=(10, 6))
plt.bar(model_names, accuracies, color='skyblue')
plt.xlabel('Models')
plt.ylabel('Accuracy')
plt.title('Model Accuracy Comparison')
plt.ylim(0, 1.0)  # Set the y-axis limit between 0 and 1
plt.xticks(rotation=45)  # Rotate x-axis labels for better visibility
plt.tight_layout()

# Display the plot
plt.show()

from sklearn.metrics import confusion_matrix, roc_curve, auc
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Replace y_true and y_scores with your actual true labels and predicted scores/probabilities for each model

y_true = [0, 1, 0, 1, 0, 1, 0, 1]
y_scores = {
    'KNN': np.array([0.8, 0.2, 0.6, 0.4, 0.7, 0.3, 0.5, 0.1]),
    'XGBoost': np.array([0.9, 0.1, 0.7, 0.3, 0.8, 0.2, 0.6, 0.4]),
    'NN': np.array([0.7, 0.3, 0.5, 0.5, 0.6, 0.4, 0.8, 0.2]),
    'Logistic Regression': np.array([0.6, 0.4, 0.8, 0.2, 0.7, 0.3, 0.5, 0.1]),
    'Decision Tree': np.array([0.5, 0.5, 0.7, 0.3, 0.6, 0.4, 0.8, 0.2]),
    'Naive Bayes': np.array([0.4, 0.6, 0.2, 0.8, 0.3, 0.7, 0.1, 0.9]),
    'Ensemble': np.array([0.8, 0.2, 0.6, 0.4, 0.7, 0.3, 0.5, 0.1]),
    'CNN': np.array([0.9, 0.1, 0.7, 0.3, 0.8, 0.2, 0.6, 0.4])
}

models = ['KNN', 'XGBoost', 'NN', 'Logistic Regression', 'Decision Tree', 'Naive Bayes', 'Ensemble', 'CNN']

plt.figure(figsize=(18, 12))

for i, model in enumerate(models, 1):
    plt.subplot(2, 4, i)

    # Calculate confusion matrix
    y_pred = (y_scores[model] > 0.5).astype(int)  # Assuming threshold is 0.5
    cm = confusion_matrix(y_true, y_pred)

    # Plot confusion matrix
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, annot_kws={"size": 16})
    plt.title(f'Confusion Matrix - {model}')
    plt.xlabel('Predicted')
    plt.ylabel('True')

    # Calculate ROC curve
    fpr, tpr, thresholds = roc_curve(y_true, y_scores[model])
    roc_auc = auc(fpr, tpr)

    # Plot ROC curve
    plt.figure(figsize=(12, 8))
    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'Model AUC = {roc_auc:.2f}')
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlabel('False Positive Rate (1 - Specificity)')
    plt.ylabel('True Positive Rate (Sensitivity)')
    plt.title(f'ROC Curve - {model}')
    plt.legend(loc='lower right')
    plt.show()

# Plotting all ROC curves in a single figure
plt.figure(figsize=(12, 8))
for model in models:
    fpr, tpr, _ = roc_curve(y_true, y_scores[model])
    roc_auc = auc(fpr, tpr)
    plt.plot(fpr, tpr, lw=2, label=f'{model} (AUC = {roc_auc:.2f})')

plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlabel('False Positive Rate (1 - Specificity)')
plt.ylabel('True Positive Rate (Sensitivity)')
plt.title('Combined ROC Curves')
plt.legend(loc='lower right')
plt.show()

from sklearn.metrics import confusion_matrix, roc_curve, auc
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Replace y_true and y_scores with your actual true labels and predicted scores/probabilities for each model

models = ['KNN', 'XGBoost', 'NN', 'Logistic Regression', 'Decision Tree', 'Naive Bayes', 'Ensemble', 'CNN']

plt.figure(figsize=(18, 12))

for i, model in enumerate(models, 1):
    plt.subplot(2, 4, i)

    # Calculate confusion matrix
    y_pred = (y_scores[model] > 0.5).astype(int)  # Assuming threshold is 0.5
    cm = confusion_matrix(y_true, y_pred)

    # Plot confusion matrix
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, annot_kws={"size": 16})
    plt.title(f'Confusion Matrix - {model}')
    plt.xlabel('Predicted')
    plt.ylabel('True')

    # Calculate ROC curve
    fpr, tpr, thresholds = roc_curve(y_true, y_scores[model])
    roc_auc = auc(fpr, tpr)

    # Plot ROC curve
    plt.figure(figsize=(12, 8))
    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'Model AUC = {roc_auc:.2f}')
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlabel('False Positive Rate (1 - Specificity)')
    plt.ylabel('True Positive Rate (Sensitivity)')
    plt.title(f'ROC Curve - {model}')
    plt.legend(loc='lower right')
    plt.show()

from sklearn.metrics import confusion_matrix, roc_curve, auc
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Replace y_true and y_scores with your actual true labels and predicted scores/probabilities for each model

y_true = [0, 1, 0, 1, 0, 1, 0, 1]
y_scores = {
    'KNN': np.array([0.8, 0.2, 0.6, 0.4, 0.7, 0.3, 0.5, 0.1]),
    'XGBoost': np.array([0.9, 0.1, 0.7, 0.3, 0.8, 0.2, 0.6, 0.4]),
    'NN': np.array([0.7, 0.3, 0.5, 0.5, 0.6, 0.4, 0.8, 0.2]),
    'Logistic Regression': np.array([0.6, 0.4, 0.8, 0.2, 0.7, 0.3, 0.5, 0.1]),
    'Decision Tree': np.array([0.5, 0.5, 0.7, 0.3, 0.6, 0.4, 0.8, 0.2]),
    'Naive Bayes': np.array([0.4, 0.6, 0.2, 0.8, 0.3, 0.7, 0.1, 0.9]),
    'Ensemble': np.array([0.8, 0.2, 0.6, 0.4, 0.7, 0.3, 0.5, 0.1]),
    'CNN': np.array([0.9, 0.1, 0.7, 0.3, 0.8, 0.2, 0.6, 0.4])
}

models = ['KNN', 'XGBoost', 'NN', 'Logistic Regression', 'Decision Tree', 'Naive Bayes', 'Ensemble', 'CNN']

# Plotting all ROC curves in a single figure
plt.figure(figsize=(12, 8))

colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k', 'purple']  # Add more colors if needed

for i, model in enumerate(models):
    fpr, tpr, _ = roc_curve(y_true, y_scores[model])
    roc_auc = auc(fpr, tpr)
    plt.plot(fpr, tpr, lw=2, label=f'{model} (AUC = {roc_auc:.2f})', color=colors[i], alpha=0.7)

plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlabel('False Positive Rate (1 - Specificity)')
plt.ylabel('True Positive Rate (Sensitivity)')
plt.title('Combined ROC Curves')
plt.legend(loc='lower right', bbox_to_anchor=(1.35, 0))
plt.show()

from sklearn.metrics import confusion_matrix, roc_curve, auc, classification_report
import matplotlib.pyplot as plt
import seaborn as sns

# Replace these lists with your actual data
y_true = [0, 1, 0, 1, 0, 1, 0, 1]

y_pred_knn = [0, 0, 0, 0, 1, 1, 0, 0]
y_pred_xgboost = [1, 0, 1, 0, 1, 0, 1, 0]
y_pred_mlp = [1, 0, 1, 1, 1, 0, 1, 0]
y_pred_logreg = [1, 0, 1, 0, 1, 0, 0, 0]
y_pred_decision_tree = [1, 0, 1, 0, 1, 1, 0, 1]
y_pred_naive_bayes = [1, 0, 0, 1, 0, 1, 0, 1]
y_pred_ensemble = [0, 0, 1, 0, 1, 1, 0, 1]  # Replace with actual predictions for the ensemble
y_pred_cnn = [1, 0, 1, 0, 1, 0, 1, 0]  # Replace with actual predictions for CNN

# Confusion Matrix and Classification Report for K-Nearest Neighbors
cm_knn = confusion_matrix(y_true, y_pred_knn)
print("Confusion Matrix (K-Nearest Neighbors):")
print(cm_knn)
print("Classification Report (K-Nearest Neighbors):")
print(classification_report(y_true, y_pred_knn))

# Repeat the above steps for each model (XGBoost, Multi-layer Perceptron, Logistic Regression, Decision Tree, Naive Bayes, Ensemble, CNN)

# Confusion Matrix and Classification Report for Ensemble
cm_ensemble = confusion_matrix(y_true, y_pred_ensemble)
print("Confusion Matrix (Ensemble):")
print(cm_ensemble)
print("Classification Report (Ensemble):")
print(classification_report(y_true, y_pred_ensemble))

# Confusion Matrix and Classification Report for CNN
cm_cnn = confusion_matrix(y_true, y_pred_cnn)
print("Confusion Matrix (CNN):")
print(cm_cnn)
print("Classification Report (CNN):")
print(classification_report(y_true, y_pred_cnn))

# Plot ROC Curves for all models
plt.figure(figsize=(10, 6))

models = ['K-Nearest Neighbors', 'XGBoost', 'Multi-layer Perceptron', 'Logistic Regression', 'Decision Tree', 'Naive Bayes', 'Ensemble', 'CNN']
predictions = [y_pred_knn, y_pred_xgboost, y_pred_mlp, y_pred_logreg, y_pred_decision_tree, y_pred_naive_bayes, y_pred_ensemble, y_pred_cnn]

for model, y_pred in zip(models, predictions):
    fpr, tpr, _ = roc_curve(y_true, y_pred)
    roc_auc = auc(fpr, tpr)
    plt.plot(fpr, tpr, lw=2, label=f'{model} (AUC = {roc_auc:.2f})')

plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlabel('False Positive Rate (1 - Specificity)')
plt.ylabel('True Positive Rate (Sensitivity)')
plt.title('ROC Curves for Different Models')
plt.legend(loc='lower right', bbox_to_anchor=(1.4, 0))
plt.show()

from sklearn.metrics import confusion_matrix, roc_curve, auc, classification_report
import matplotlib.pyplot as plt
import seaborn as sns

# Replace these lists with your actual data
y_true = [0, 1, 0, 1, 0, 1, 0, 1]

y_pred_knn = [0, 0, 0, 0, 1, 1, 0, 0]
y_pred_xgboost = [1, 0, 1, 0, 1, 0, 1, 0]
y_pred_mlp = [1, 0, 1, 1, 1, 0, 1, 0]
y_pred_logreg = [1, 0, 1, 0, 1, 0, 0, 0]
y_pred_decision_tree = [1, 0, 1, 0, 1, 1, 0, 1]
y_pred_naive_bayes = [1, 0, 0, 1, 0, 1, 0, 1]
y_pred_ensemble = [0, 0, 1, 0, 1, 1, 0, 1]
y_pred_cnn = [1, 0, 1, 0, 1, 0, 1, 0]

models = ['K-Nearest Neighbors', 'XGBoost', 'Multi-layer Perceptron', 'Logistic Regression', 'Decision Tree', 'Naive Bayes', 'Ensemble', 'CNN']
predictions = [y_pred_knn, y_pred_xgboost, y_pred_mlp, y_pred_logreg, y_pred_decision_tree, y_pred_naive_bayes, y_pred_ensemble, y_pred_cnn]

# Plot individual confusion matrices
plt.figure(figsize=(15, 10))
for i, (model, y_pred) in enumerate(zip(models, predictions), 1):
    plt.subplot(3, 3, i)
    cm = confusion_matrix(y_true, y_pred)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)
    plt.title(f'Confusion Matrix ({model})')
    plt.xlabel('Predicted')
    plt.ylabel('True')

plt.tight_layout()
plt.show()

# Plot individual ROC curves
plt.figure(figsize=(10, 6))
for model, y_pred in zip(models, predictions):
    fpr, tpr, _ = roc_curve(y_true, y_pred)
    roc_auc = auc(fpr, tpr)
    plt.plot(fpr, tpr, lw=2, label=f'{model} (AUC = {roc_auc:.2f})')

plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlabel('False Positive Rate (1 - Specificity)')
plt.ylabel('True Positive Rate (Sensitivity)')
plt.title('ROC Curves for Different Models')
plt.legend(loc='lower right')
plt.show()

from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# Replace these lists with your actual data
y_true = [0, 1, 0, 1, 0, 1, 0, 1]

y_pred_knn = [0, 0, 0, 0, 1, 1, 0, 0]
y_pred_xgboost = [1, 0, 1, 0, 1, 0, 1, 0]
y_pred_mlp = [1, 0, 1, 1, 1, 0, 1, 0]
y_pred_logreg = [1, 0, 1, 0, 1, 0, 0, 0]
y_pred_decision_tree = [1, 0, 1, 0, 1, 1, 0, 1]
y_pred_naive_bayes = [1, 0, 0, 1, 0, 1, 0, 1]
y_pred_ensemble = [0, 0, 1, 0, 1, 1, 0, 1]  # Replace with actual predictions for the ensemble
y_pred_cnn = [1, 0, 1, 0, 1, 0, 1, 0]  # Replace with actual predictions for CNN

models = ['K-Nearest Neighbors', 'XGBoost', 'Multi-layer Perceptron', 'Logistic Regression', 'Decision Tree', 'Naive Bayes', 'Ensemble', 'CNN']
predictions = [y_pred_knn, y_pred_xgboost, y_pred_mlp, y_pred_logreg, y_pred_decision_tree, y_pred_naive_bayes, y_pred_ensemble, y_pred_cnn]

# Plot individual confusion matrices one by one
for model, y_pred in zip(models, predictions):
    plt.figure(figsize=(6, 6))
    cm = confusion_matrix(y_true, y_pred)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)
    plt.title(f'Confusion Matrix\n({model})')
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.show()

from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt

# Replace these lists with your actual data
y_true = [0, 1, 0, 1, 0, 1, 0, 1]

y_scores_knn = [0.8, 0.2, 0.6, 0.4, 0.7, 0.3, 0.5, 0.1]
y_scores_xgboost = [0.9, 0.1, 0.7, 0.3, 0.8, 0.2, 0.6, 0.4]
y_scores_mlp = [0.7, 0.3, 0.5, 0.5, 0.6, 0.4, 0.8, 0.2]
y_scores_logreg = [0.6, 0.4, 0.8, 0.2, 0.7, 0.3, 0.5, 0.1]
y_scores_decision_tree = [0.5, 0.5, 0.7, 0.3, 0.6, 0.4, 0.8, 0.2]
y_scores_naive_bayes = [0.4, 0.6, 0.2, 0.8, 0.3, 0.7, 0.1, 0.9]
y_scores_ensemble = [0.8, 0.2, 0.6, 0.4, 0.7, 0.3, 0.5, 0.1]
y_scores_cnn = [0.9, 0.1, 0.7, 0.3, 0.8, 0.2, 0.6, 0.4]

models = ['KNN', 'XGBoost', 'MLP', 'Logistic Regression', 'Decision Tree', 'Naive Bayes', 'Ensemble', 'CNN']
predictions = [y_scores_knn, y_scores_xgboost, y_scores_mlp, y_scores_logreg, y_scores_decision_tree, y_scores_naive_bayes, y_scores_ensemble, y_scores_cnn]

# Plot individual ROC curves
plt.figure(figsize=(10, 6))
for model, y_scores in zip(models, predictions):
    fpr, tpr, _ = roc_curve(y_true, y_scores)
    roc_auc = auc(fpr, tpr)
    plt.plot(fpr, tpr, lw=2, label=f'{model} (AUC = {roc_auc:.2f})')

plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlabel('False Positive Rate (1 - Specificity)')
plt.ylabel('True Positive Rate (Sensitivity)')
plt.title('ROC Curves for Different Models')
plt.legend(loc='lower right')
plt.show()

import matplotlib.pyplot as plt
import numpy as np

# Model names
models = ['KNN', 'XGBoost', 'NN', 'LR', 'DT', 'NB', 'Ensembled', 'CNN']

# Scores for each model and configuration
scores = np.array([
    [0.92, 0.88, 0.87, 0.88],  # KNN
    [0.95, 0.94, 0.96, 0.93],  # XGBoost
    [0.97, 0.96, 0.98, 0.97],  # Neural Network
    [0.96, 0.97, 0.95, 0.96],  # Logistic Regression
    [0.88, 0.89, 0.87, 0.91],  # Decision Tree
    [0.94, 0.95, 0.96, 0.94],  # Naive Bayes
    [0.98, 0.97, 0.96, 0.97],  # Ensembled Model
    [0.92, 0.93, 0.94, 0.91]   # CNN
])

# Configurations
configurations = ['Default', 'Hyperparameter Tuning', 'Stemmer + Hyperparameter Tuning', 'Length + Stemmer + Hyperparameter Tuning']

# Plotting
fig, ax = plt.subplots(figsize=(10, 6))

for i in range(len(configurations)):
    ax.plot(models, scores[:, i], marker='o', label=f'Score {i + 1}: {configurations[i]}')

# Adding labels and title
ax.set_xlabel('Models')
ax.set_ylabel('Scores')
ax.set_title('Model Performance under Different Configurations')
ax.legend(loc='lower right', bbox_to_anchor=(1.57, 0))

# Show the plot
plt.show()